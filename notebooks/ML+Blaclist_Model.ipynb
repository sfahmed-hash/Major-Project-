{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "25Qs1LjhVFVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# HYBRID ENSEMBLE MODEL FOR TOP 20 DATASET\n",
        "# (DT + ANN + KNN with SOFT VOTING)\n",
        "# ============================================================\n",
        "\n",
        "# ============================================================\n",
        "# IMPORTS\n",
        "# ============================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "# ============================================================\n",
        "# LOAD DATASET\n",
        "# ============================================================\n",
        "df = pd.read_csv(\"/content/Dataset_useful_top20.csv\")\n",
        "\n",
        "print(\"Dataset shape:\", df.shape)\n",
        "\n",
        "# Target column\n",
        "target_col = \"Type\"\n",
        "\n",
        "# Convert to numeric\n",
        "df[target_col] = df[target_col].astype(\"category\").cat.codes\n",
        "\n",
        "# Features and labels\n",
        "X = df.drop(columns=[target_col])\n",
        "y = df[target_col]\n",
        "\n",
        "print(\"Feature shape:\", X.shape)\n",
        "\n",
        "# ============================================================\n",
        "# TRAIN-TEST SPLIT\n",
        "# ============================================================\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# DEFINE BASE MODELS\n",
        "# ============================================================\n",
        "\n",
        "# Decision Tree\n",
        "dt_model = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
        "    (\"dt\", DecisionTreeClassifier(max_depth=10, random_state=42))\n",
        "])\n",
        "\n",
        "# ANN (MLP)\n",
        "mlp_model = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"mlp\", MLPClassifier(\n",
        "        hidden_layer_sizes=(128, 64),\n",
        "        max_iter=300,\n",
        "        random_state=42,\n",
        "        early_stopping=True\n",
        "    ))\n",
        "])\n",
        "\n",
        "# KNN\n",
        "knn_model = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"knn\", KNeighborsClassifier(n_neighbors=5))\n",
        "])\n",
        "\n",
        "# ============================================================\n",
        "# HYBRID ENSEMBLE MODEL (SOFT VOTING)\n",
        "# ============================================================\n",
        "hybrid_model = VotingClassifier(\n",
        "    estimators=[\n",
        "        (\"dt\", dt_model),\n",
        "        (\"mlp\", mlp_model),\n",
        "        (\"knn\", knn_model)\n",
        "    ],\n",
        "    voting=\"soft\"   # probability-based voting\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# TRAIN MODEL\n",
        "# ============================================================\n",
        "print(\"\\nTraining Hybrid Model...\")\n",
        "hybrid_model.fit(X_train, y_train)\n",
        "\n",
        "# ============================================================\n",
        "# EVALUATION\n",
        "# ============================================================\n",
        "y_pred = hybrid_model.predict(X_test)\n",
        "y_proba = hybrid_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"\\nAccuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# ============================================================\n",
        "# CONFIDENCE PREDICTION FUNCTION\n",
        "# ============================================================\n",
        "def predict_sample(sample):\n",
        "\n",
        "    sample = np.array(sample).reshape(1, -1)\n",
        "    prob = hybrid_model.predict_proba(sample)[0][1]\n",
        "\n",
        "    if prob >= 0.5:\n",
        "        return \"‚ö†Ô∏è Phishing\", prob\n",
        "    else:\n",
        "        return \"‚úÖ Legitimate\", prob\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# TEST USING DATASET SAMPLE\n",
        "# ============================================================\n",
        "print(\"\\nTesting on sample row:\")\n",
        "sample = X.iloc[0].values\n",
        "\n",
        "print(\"Actual Label:\", y.iloc[0])\n",
        "print(\"Prediction:\", predict_sample(sample))\n",
        "\n",
        "import joblib\n",
        "\n",
        "joblib.dump(hybrid_model, \"/content/hybrid_model.pkl\")\n",
        "\n",
        "print(\"Model saved!\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nNNWtdBfVLi1",
        "outputId": "1fdd8ddf-28c2-4d20-f6e2-af8fb417e832"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (247950, 21)\n",
            "Feature shape: (247950, 20)\n",
            "\n",
            "Training Hybrid Model...\n",
            "\n",
            "Accuracy: 0.9192780802581165\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.95      0.92     25708\n",
            "           1       0.94      0.89      0.91     23882\n",
            "\n",
            "    accuracy                           0.92     49590\n",
            "   macro avg       0.92      0.92      0.92     49590\n",
            "weighted avg       0.92      0.92      0.92     49590\n",
            "\n",
            "\n",
            "Testing on sample row:\n",
            "Actual Label: 0\n",
            "Prediction: ('‚úÖ Legitimate', np.float64(0.024253054005665967))\n",
            "Model saved!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but SimpleImputer was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but SimpleImputer was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but SimpleImputer was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3NbBQhuitSQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tldextract python-whois dnspython joblib -q\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXNPQAaDrS0F",
        "outputId": "bc7da219-73ed-41f6-9dbc-99e7e9ab81b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/105.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m105.9/105.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/117.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/331.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m331.1/331.1 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Phishing-Database/Phishing.Database.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GycV7UPwrmh4",
        "outputId": "0758f95a-e180-4053-c062-2b5c004008f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Phishing.Database'...\n",
            "remote: Enumerating objects: 15636, done.\u001b[K\n",
            "remote: Counting objects: 100% (8/8), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 15636 (delta 3), reused 3 (delta 2), pack-reused 15628 (from 3)\u001b[K\n",
            "Receiving objects: 100% (15636/15636), 1.30 GiB | 22.24 MiB/s, done.\n",
            "Resolving deltas: 100% (12409/12409), done.\n",
            "Updating files: 100% (69/69), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------\n",
        "# BLACKLIST_CHECKER.PY\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "import os\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "def load_blacklist_domains(path):\n",
        "    domains = set()\n",
        "\n",
        "    for root, dirs, files in os.walk(path):\n",
        "        for file in files:\n",
        "            if file.endswith(\".txt\"):\n",
        "                with open(os.path.join(root, file), \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "                    for line in f:\n",
        "                        entry = line.strip()\n",
        "                        domain = urlparse(entry).netloc\n",
        "\n",
        "                        if not domain:\n",
        "                            domain = entry\n",
        "\n",
        "                        if domain:\n",
        "                            domains.add(domain.lower())\n",
        "\n",
        "    print(\"Loaded domains:\", len(domains))\n",
        "    return domains\n",
        "\n",
        "\n",
        "blacklist_domains = load_blacklist_domains(\"/content/Phishing.Database\")\n",
        "\n",
        "def is_blacklisted(url):\n",
        "    domain = urlparse(url).netloc.lower()\n",
        "\n",
        "    if domain.startswith(\"www.\"):\n",
        "        domain = domain[4:]\n",
        "\n",
        "    return domain in blacklist_domains\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-ALTZTvtTXZ",
        "outputId": "e780e5be-c5a0-438f-fa16-69996d1a0306"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded domains: 945057\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------\n",
        "# FEATURE_EXTRACTOR.PY\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "import re\n",
        "import math\n",
        "from urllib.parse import urlparse\n",
        "import tldextract\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# HELPER: ENTROPY CALCULATION\n",
        "# ------------------------------------------------------------\n",
        "def calculate_entropy(string):\n",
        "    if not string:\n",
        "        return 0\n",
        "\n",
        "    prob = [float(string.count(c)) / len(string) for c in set(string)]\n",
        "    entropy = -sum([p * math.log2(p) for p in prob])\n",
        "\n",
        "    return entropy\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# MAIN FEATURE EXTRACTION FUNCTION\n",
        "# ------------------------------------------------------------\n",
        "def extract_features(url):\n",
        "\n",
        "    url = str(url)\n",
        "    parsed = urlparse(url)\n",
        "    ext = tldextract.extract(url)\n",
        "\n",
        "    domain = ext.domain + \".\" + ext.suffix\n",
        "    subdomain = ext.subdomain\n",
        "    path = parsed.path\n",
        "\n",
        "    features = {}\n",
        "\n",
        "    # --------------------------------------------------------\n",
        "    # BASIC LENGTH FEATURES\n",
        "    # --------------------------------------------------------\n",
        "    features[\"url_length\"] = len(url)\n",
        "    features[\"domain_length\"] = len(domain)\n",
        "    features[\"path_length\"] = len(path)\n",
        "\n",
        "    # --------------------------------------------------------\n",
        "    # SUBDOMAIN FEATURES\n",
        "    # --------------------------------------------------------\n",
        "    subdomains = subdomain.split('.') if subdomain else []\n",
        "\n",
        "    features[\"number_of_subdomains\"] = len(subdomains)\n",
        "\n",
        "    if subdomains:\n",
        "        avg_len = sum(len(s) for s in subdomains) / len(subdomains)\n",
        "    else:\n",
        "        avg_len = 0\n",
        "\n",
        "    features[\"average_subdomain_length\"] = avg_len\n",
        "\n",
        "    # Digits in subdomain\n",
        "    features[\"number_of_digits_in_subdomain\"] = sum(\n",
        "        sum(c.isdigit() for c in s) for s in subdomains\n",
        "    )\n",
        "\n",
        "    # --------------------------------------------------------\n",
        "    # ENTROPY FEATURES\n",
        "    # --------------------------------------------------------\n",
        "    features[\"entropy_of_url\"] = calculate_entropy(url)\n",
        "    features[\"entropy_of_domain\"] = calculate_entropy(domain)\n",
        "\n",
        "    # --------------------------------------------------------\n",
        "    # CHARACTER COUNTS\n",
        "    # --------------------------------------------------------\n",
        "    features[\"number_of_special_char_in_url\"] = len(re.findall(r'[^a-zA-Z0-9]', url))\n",
        "    features[\"number_of_digits_in_url\"] = sum(c.isdigit() for c in url)\n",
        "    features[\"number_of_digits_in_domain\"] = sum(c.isdigit() for c in domain)\n",
        "\n",
        "    # --------------------------------------------------------\n",
        "    # DOT / SLASH / SYMBOL COUNTS\n",
        "    # --------------------------------------------------------\n",
        "    features[\"number_of_dots_in_domain\"] = domain.count('.')\n",
        "    features[\"number_of_dots_in_url\"] = url.count('.')\n",
        "    features[\"number_of_slash_in_url\"] = url.count('/')\n",
        "    features[\"number_of_equal_in_url\"] = url.count('=')\n",
        "    features[\"number_of_questionmark_in_url\"] = url.count('?')\n",
        "\n",
        "    # --------------------------------------------------------\n",
        "    # HYPHENS\n",
        "    # --------------------------------------------------------\n",
        "    features[\"number_of_hyphens_in_domain\"] = domain.count('-')\n",
        "    features[\"number_of_hyphens_in_url\"] = url.count('-')\n",
        "\n",
        "    # --------------------------------------------------------\n",
        "    # DIGIT FLAGS\n",
        "    # --------------------------------------------------------\n",
        "    features[\"having_digits_in_domain\"] = int(any(c.isdigit() for c in domain))\n",
        "\n",
        "    # Repeated digits in domain (e.g., 111, 222)\n",
        "    digit_counts = Counter(c for c in domain if c.isdigit())\n",
        "    features[\"having_repeated_digits_in_domain\"] = int(\n",
        "        any(count > 1 for count in digit_counts.values())\n",
        "    )\n",
        "\n",
        "    return features\n"
      ],
      "metadata": {
        "id": "YpIIkr_cuA3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "model = joblib.load(\"/content/hybrid_model.pkl\")\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "FEATURE_COLUMNS = [\n",
        "    \"url_length\",\n",
        "    \"average_subdomain_length\",\n",
        "    \"entropy_of_url\",\n",
        "    \"entropy_of_domain\",\n",
        "    \"domain_length\",\n",
        "    \"number_of_subdomains\",\n",
        "    \"number_of_special_char_in_url\",\n",
        "    \"number_of_digits_in_url\",\n",
        "    \"number_of_digits_in_domain\",\n",
        "    \"number_of_dots_in_domain\",\n",
        "    \"number_of_slash_in_url\",\n",
        "    \"number_of_dots_in_url\",\n",
        "    \"path_length\",\n",
        "    \"number_of_hyphens_in_domain\",\n",
        "    \"number_of_hyphens_in_url\",\n",
        "    \"having_digits_in_domain\",\n",
        "    \"number_of_equal_in_url\",\n",
        "    \"number_of_digits_in_subdomain\",\n",
        "    \"having_repeated_digits_in_domain\",\n",
        "    \"number_of_questionmark_in_url\"\n",
        "]\n",
        "\n",
        "def predict_url(url):\n",
        "\n",
        "    # 1Ô∏è‚É£ BLACKLIST\n",
        "    if is_blacklisted(url):\n",
        "        return \"üö® Phishing (Blacklist)\", 1.0\n",
        "\n",
        "    # 2Ô∏è‚É£ ML\n",
        "    features = extract_features(url)\n",
        "    df = pd.DataFrame([features])\n",
        "    df = df.reindex(columns=FEATURE_COLUMNS, fill_value=0)\n",
        "\n",
        "    prob = model.predict_proba(df)[0][1]\n",
        "\n",
        "    if prob >= 0.5:\n",
        "        return \"‚ö†Ô∏è Phishing (ML)\", prob\n",
        "    else:\n",
        "        return \"‚úÖ Legitimate\", prob\n",
        "\n",
        "print(predict_url(\"https://www.google.com\"))\n",
        "print(predict_url(\"http://secure-login-bank.xyz\"))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BH3YBD3LuG-B",
        "outputId": "80c2cffa-c2ef-4bf0-8014-676357d76d9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('‚úÖ Legitimate', np.float64(0.0005390283725638406))\n",
            "('‚ö†Ô∏è Phishing (ML)', np.float64(0.6610612835518214))\n"
          ]
        }
      ]
    }
  ]
}